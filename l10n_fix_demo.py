#!/usr/bin/env python3
"""
ç®€å•çš„æœ¬åœ°åŒ–ä¿®å¤æ¼”ç¤º - å¤„ç†å…·ä½“æ–‡ä»¶çš„å¯¼å…¥å’Œç¡¬ç¼–ç æ–‡æœ¬æ›¿æ¢
"""

import os
import re

def analyze_dart_file(file_path):
    """åˆ†æDartæ–‡ä»¶ï¼Œæ£€æŸ¥æœ¬åœ°åŒ–çŠ¶æ€"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    analysis = {
        'has_material_import': 'package:flutter/material.dart' in content,
        'has_l10n_import': any([
            'package:flutter_gen/gen_l10n/app_localizations.dart' in content,
            'generated/l10n/l10n.dart' in content,
            'from \'../../../generated/l10n.dart\'' in content
        ]),
        'uses_s_of_context': 'S.of(context)' in content,
        'hardcoded_chinese': [],
        'existing_imports': []
    }
    
    # æŸ¥æ‰¾ç¡¬ç¼–ç ä¸­æ–‡æ–‡æœ¬
    chinese_pattern = r'[\'"][^\'\"]*[\u4e00-\u9fff][^\'\"]*[\'"]'
    matches = re.finditer(chinese_pattern, content)
    for match in matches:
        text = match.group()
        line_num = content[:match.start()].count('\n') + 1
        analysis['hardcoded_chinese'].append({
            'text': text,
            'line': line_num,
            'start': match.start(),
            'end': match.end()
        })
    
    # æå–å¯¼å…¥è¯­å¥
    import_lines = []
    for i, line in enumerate(content.split('\n'), 1):
        if line.strip().startswith('import '):
            import_lines.append((i, line.strip()))
    analysis['existing_imports'] = import_lines
    
    return analysis

def add_l10n_import(content):
    """æ·»åŠ æœ¬åœ°åŒ–å¯¼å…¥"""
    lines = content.split('\n')
    
    # æŸ¥æ‰¾åˆé€‚çš„æ’å…¥ä½ç½®
    last_import_index = -1
    for i, line in enumerate(lines):
        if line.strip().startswith('import '):
            last_import_index = i
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰l10nå¯¼å…¥
    l10n_imports = [
        "import '../../../generated/l10n.dart';",
        "import 'package:flutter_gen/gen_l10n/app_localizations.dart';"
    ]
    
    has_l10n = any(any(imp in line for imp in l10n_imports) for line in lines)
    
    if not has_l10n:
        # æ·»åŠ æœ¬åœ°åŒ–å¯¼å…¥
        insert_pos = last_import_index + 1 if last_import_index >= 0 else 0
        lines.insert(insert_pos, "import '../../../generated/l10n.dart';")
        print(f"  ğŸ“¦ æ·»åŠ æœ¬åœ°åŒ–å¯¼å…¥åˆ°ç¬¬ {insert_pos + 1} è¡Œ")
    else:
        print("  ğŸ“¦ æœ¬åœ°åŒ–å¯¼å…¥å·²å­˜åœ¨")
    
    return '\n'.join(lines)

def fix_hardcoded_text(content, hardcoded_items, arb_mappings):
    """æ›¿æ¢ç¡¬ç¼–ç æ–‡æœ¬"""
    # æŒ‰ä½ç½®å€’åºæ’åˆ—ï¼Œé¿å…æ›¿æ¢åä½ç½®åç§»
    sorted_items = sorted(hardcoded_items, key=lambda x: x['start'], reverse=True)
    
    for item in sorted_items:
        text = item['text']
        clean_text = text[1:-1]  # ç§»é™¤å¼•å·
        
        # æŸ¥æ‰¾å¯¹åº”çš„ARBé”®
        arb_key = arb_mappings.get(clean_text)
        if arb_key:
            replacement = f"S.of(context).{arb_key}"
            content = content[:item['start']] + replacement + content[item['end']:]
            print(f"  âœ… ç¬¬ {item['line']} è¡Œ: {text} -> {replacement}")
        else:
            print(f"  âš ï¸  ç¬¬ {item['line']} è¡Œ: {text} - æœªæ‰¾åˆ°ARBæ˜ å°„")
    
    return content

def demonstrate_l10n_fix():
    """æ¼”ç¤ºæœ¬åœ°åŒ–ä¿®å¤è¿‡ç¨‹"""
    file_path = "lib/presentation/widgets/works/preview_mode_config.dart"
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    print(f"\nğŸ” åˆ†ææ–‡ä»¶: {file_path}")
    
    # åˆ†ææ–‡ä»¶
    analysis = analyze_dart_file(file_path)
    
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"  Materialå¯¼å…¥: {'âœ…' if analysis['has_material_import'] else 'âŒ'}")
    print(f"  æœ¬åœ°åŒ–å¯¼å…¥: {'âœ…' if analysis['has_l10n_import'] else 'âŒ'}")
    print(f"  ä½¿ç”¨S.of(context): {'âœ…' if analysis['uses_s_of_context'] else 'âŒ'}")
    print(f"  ç¡¬ç¼–ç ä¸­æ–‡: {len(analysis['hardcoded_chinese'])} ä¸ª")
    
    if analysis['hardcoded_chinese']:
        print(f"\nğŸ“ å‘ç°çš„ç¡¬ç¼–ç ä¸­æ–‡:")
        for item in analysis['hardcoded_chinese']:
            print(f"  ç¬¬ {item['line']} è¡Œ: {item['text']}")
    
    # ç¤ºä¾‹ARBæ˜ å°„
    arb_mappings = {
        "ä¿å­˜æ›´æ”¹": "saveChanges",
        "æ·»åŠ å›¾ç‰‡": "addImage", 
        "åˆ é™¤å›¾ç‰‡": "deleteImage"
    }
    
    # è¯»å–åŸå§‹å†…å®¹
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"\nğŸ”§ å¼€å§‹ä¿®å¤:")
    
    # æ·»åŠ å¯¼å…¥
    if not analysis['has_l10n_import']:
        content = add_l10n_import(content)
    
    # æ›¿æ¢ç¡¬ç¼–ç æ–‡æœ¬
    if analysis['hardcoded_chinese']:
        content = fix_hardcoded_text(content, analysis['hardcoded_chinese'], arb_mappings)
    
    # ä¿å­˜ä¿®å¤åçš„å†…å®¹ï¼ˆæ¼”ç¤ºç”¨ï¼‰
    output_path = f"{file_path}.fixed_demo"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"\nâœ… ä¿®å¤å®Œæˆï¼æ¼”ç¤ºç»“æœä¿å­˜åˆ°: {output_path}")
    print(f"\nğŸ“‹ ä¿®å¤æ€»ç»“:")
    print(f"  - å·²ç¡®ä¿æœ¬åœ°åŒ–å¯¼å…¥å­˜åœ¨")
    print(f"  - å·²æ›¿æ¢ {len([item for item in analysis['hardcoded_chinese'] if item['text'][1:-1] in arb_mappings])} ä¸ªç¡¬ç¼–ç æ–‡æœ¬")
    
    # æ˜¾ç¤ºä¿®å¤åçš„å…³é”®éƒ¨åˆ†
    print(f"\nğŸ“„ ä¿®å¤åçš„å…³é”®éƒ¨åˆ†:")
    lines = content.split('\n')
    for i, line in enumerate(lines[:10], 1):
        if 'import' in line or 'S.of(context)' in line:
            print(f"  {i:2d}: {line}")

if __name__ == "__main__":
    demonstrate_l10n_fix()
