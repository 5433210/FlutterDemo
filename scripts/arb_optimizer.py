#!/usr/bin/env python3
"""
ARBæ–‡ä»¶ä¼˜åŒ–å™¨
ç”¨äºåˆ†æã€ä¼˜åŒ–ARBæ–‡ä»¶ï¼Œåˆ é™¤é‡å¤é”®å€¼ï¼Œåˆå¹¶ç›¸ä¼¼é”®å€¼ï¼Œæ¸…ç†æ— ç”¨é”®å€¼
"""

import os
import json
import re
import argparse
import subprocess
from collections import OrderedDict, defaultdict
from difflib import SequenceMatcher
from datetime import datetime
import shutil

class ARBOptimizer:
    def __init__(self, l10n_dir="lib/l10n"):
        self.l10n_dir = l10n_dir
        self.zh_arb_path = os.path.join(l10n_dir, "app_zh.arb")
        self.en_arb_path = os.path.join(l10n_dir, "app_en.arb")
        self.backup_dir = f"arb_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
    def load_arb_files(self):
        """åŠ è½½ARBæ–‡ä»¶"""
        try:
            with open(self.zh_arb_path, 'r', encoding='utf-8') as f:
                zh_data = json.load(f, object_pairs_hook=OrderedDict)
            
            with open(self.en_arb_path, 'r', encoding='utf-8') as f:
                en_data = json.load(f, object_pairs_hook=OrderedDict)
                
            return zh_data, en_data
        except Exception as e:
            print(f"âŒ åŠ è½½ARBæ–‡ä»¶å¤±è´¥: {e}")
            return None, None
    
    def find_dart_files(self):
        """æŸ¥æ‰¾æ‰€æœ‰Dartæ–‡ä»¶"""
        dart_files = []
        for root, dirs, files in os.walk("lib"):
            for file in files:
                if file.endswith('.dart'):
                    dart_files.append(os.path.join(root, file))
        return dart_files
    
    def find_used_keys(self):
        """æŸ¥æ‰¾ä»£ç ä¸­ä½¿ç”¨çš„ARBé”®å€¼"""
        used_keys = set()
        dart_files = self.find_dart_files()
        
        # å¸¸è§çš„æœ¬åœ°åŒ–å¼•ç”¨æ¨¡å¼
        patterns = [
            r'AppLocalizations\.of\(context\)\.(\w+)',
            r'l10n\.(\w+)',
            r'localizations\.(\w+)',
            r'_localizations\.(\w+)',
        ]
        
        for file_path in dart_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                for pattern in patterns:
                    matches = re.findall(pattern, content)
                    used_keys.update(matches)
                    
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                
        return used_keys
    
    def calculate_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()
    
    def find_duplicate_keys(self, zh_data, en_data):
        """æŸ¥æ‰¾é‡å¤æˆ–ç›¸ä¼¼çš„é”®å€¼"""
        duplicates = []
        keys = [k for k in zh_data.keys() if not k.startswith('@')]
        
        for i, key1 in enumerate(keys):
            for key2 in keys[i+1:]:
                zh_sim = self.calculate_similarity(zh_data[key1], zh_data[key2])
                en_sim = self.calculate_similarity(en_data.get(key2, ''), en_data.get(key1, ''))
                
                # å¦‚æœä¸­æ–‡æˆ–è‹±æ–‡ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                if zh_sim > 0.85 or en_sim > 0.85:
                    duplicates.append({
                        'key1': key1,
                        'key2': key2,
                        'zh_text1': zh_data[key1],
                        'zh_text2': zh_data[key2],
                        'en_text1': en_data.get(key1, ''),
                        'en_text2': en_data.get(key2, ''),
                        'zh_similarity': zh_sim,
                        'en_similarity': en_sim
                    })
        
        return duplicates
    
    def find_unused_keys(self, zh_data, used_keys):
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„é”®å€¼"""
        all_keys = set(k for k in zh_data.keys() if not k.startswith('@'))
        unused_keys = all_keys - used_keys
        return unused_keys
    
    def find_poorly_named_keys(self, zh_data):
        """æŸ¥æ‰¾å‘½åä¸è§„èŒƒçš„é”®å€¼"""
        poorly_named = []
        
        # ä¸è‰¯å‘½åæ¨¡å¼
        bad_patterns = [
            r'^label\d*$',      # label, label1, label2
            r'^text\d*$',       # text, text1, text2
            r'^title\d*$',      # title1, title2 (ä½†ä¿ç•™title)
            r'^msg\d*$',        # msg1, msg2
            r'^str\d*$',        # str1, str2
            r'^temp\w*$',       # temp, temporary
            r'^test\w*$',       # testç›¸å…³
        ]
        
        for key in zh_data.keys():
            if key.startswith('@'):
                continue
                
            for pattern in bad_patterns:
                if re.match(pattern, key, re.IGNORECASE):
                    poorly_named.append({
                        'key': key,
                        'zh_text': zh_data[key],
                        'reason': f'åŒ¹é…ä¸è‰¯å‘½åæ¨¡å¼: {pattern}'
                    })
                    break
        
        return poorly_named
    
    def analyze_arb_files(self):
        """åˆ†æARBæ–‡ä»¶ï¼Œç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        print("ğŸ” å¼€å§‹åˆ†æARBæ–‡ä»¶...")
        
        zh_data, en_data = self.load_arb_files()
        if not zh_data or not en_data:
            return
        
        # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
        total_keys = len([k for k in zh_data.keys() if not k.startswith('@')])
        print(f"ğŸ“Š æ€»é”®å€¼æ•°é‡: {total_keys}")
        
        # æŸ¥æ‰¾ä½¿ç”¨çš„é”®å€¼
        print("ğŸ” æŸ¥æ‰¾ä»£ç ä¸­ä½¿ç”¨çš„é”®å€¼...")
        used_keys = self.find_used_keys()
        print(f"ğŸ“Š å·²ä½¿ç”¨é”®å€¼: {len(used_keys)}")
        
        # æŸ¥æ‰¾é‡å¤é”®å€¼
        print("ğŸ” æŸ¥æ‰¾é‡å¤é”®å€¼...")
        duplicates = self.find_duplicate_keys(zh_data, en_data)
        print(f"ğŸ“Š ç–‘ä¼¼é‡å¤é”®å€¼ç»„: {len(duplicates)}")
        
        # æŸ¥æ‰¾æœªä½¿ç”¨é”®å€¼
        print("ğŸ” æŸ¥æ‰¾æœªä½¿ç”¨é”®å€¼...")
        unused_keys = self.find_unused_keys(zh_data, used_keys)
        print(f"ğŸ“Š æœªä½¿ç”¨é”®å€¼: {len(unused_keys)}")
        
        # æŸ¥æ‰¾å‘½åä¸è§„èŒƒé”®å€¼
        print("ğŸ” æŸ¥æ‰¾å‘½åä¸è§„èŒƒé”®å€¼...")
        poorly_named = self.find_poorly_named_keys(zh_data)
        print(f"ğŸ“Š å‘½åä¸è§„èŒƒé”®å€¼: {len(poorly_named)}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_analysis_report(zh_data, en_data, used_keys, duplicates, unused_keys, poorly_named)
    
    def generate_analysis_report(self, zh_data, en_data, used_keys, duplicates, unused_keys, poorly_named):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_file = "arb_analysis_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ARBæ–‡ä»¶åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # åŸºæœ¬ç»Ÿè®¡
            total_keys = len([k for k in zh_data.keys() if not k.startswith('@')])
            f.write("## åŸºæœ¬ç»Ÿè®¡\n\n")
            f.write(f"- æ€»é”®å€¼æ•°é‡: {total_keys}\n")
            f.write(f"- å·²ä½¿ç”¨é”®å€¼: {len(used_keys)}\n")
            f.write(f"- æœªä½¿ç”¨é”®å€¼: {len(unused_keys)}\n")
            f.write(f"- ç–‘ä¼¼é‡å¤é”®å€¼ç»„: {len(duplicates)}\n")
            f.write(f"- å‘½åä¸è§„èŒƒé”®å€¼: {len(poorly_named)}\n\n")
            
            # é‡å¤é”®å€¼è¯¦æƒ…
            if duplicates:
                f.write("## ç–‘ä¼¼é‡å¤é”®å€¼\n\n")
                for dup in duplicates[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                    f.write(f"### {dup['key1']} vs {dup['key2']}\n\n")
                    f.write(f"- **ä¸­æ–‡ç›¸ä¼¼åº¦**: {dup['zh_similarity']:.2f}\n")
                    f.write(f"- **è‹±æ–‡ç›¸ä¼¼åº¦**: {dup['en_similarity']:.2f}\n")
                    f.write(f"- **{dup['key1']}**: {dup['zh_text1']} / {dup['en_text1']}\n")
                    f.write(f"- **{dup['key2']}**: {dup['zh_text2']} / {dup['en_text2']}\n\n")
            
            # æœªä½¿ç”¨é”®å€¼
            if unused_keys:
                f.write("## æœªä½¿ç”¨é”®å€¼\n\n")
                for key in sorted(list(unused_keys))[:50]:  # åªæ˜¾ç¤ºå‰50ä¸ª
                    f.write(f"- **{key}**: {zh_data[key]}\n")
                f.write("\n")
            
            # å‘½åä¸è§„èŒƒé”®å€¼
            if poorly_named:
                f.write("## å‘½åä¸è§„èŒƒé”®å€¼\n\n")
                for item in poorly_named:
                    f.write(f"- **{item['key']}**: {item['zh_text']} ({item['reason']})\n")
                f.write("\n")
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def create_backup(self):
        """åˆ›å»ºARBæ–‡ä»¶å¤‡ä»½"""
        if not os.path.exists(self.backup_dir):
            os.makedirs(self.backup_dir)
        
        shutil.copy2(self.zh_arb_path, os.path.join(self.backup_dir, "app_zh.arb"))
        shutil.copy2(self.en_arb_path, os.path.join(self.backup_dir, "app_en.arb"))
        
        print(f"âœ… å¤‡ä»½å·²åˆ›å»º: {self.backup_dir}")
    
    def generate_key_mapping(self, duplicates, unused_keys):
        """ç”Ÿæˆé”®å€¼æ˜ å°„è¡¨"""
        mapping = {}
        
        # å¤„ç†é‡å¤é”®å€¼ - ä¿ç•™è¾ƒçŸ­æˆ–æ›´è¯­ä¹‰åŒ–çš„é”®å
        for dup in duplicates:
            key1, key2 = dup['key1'], dup['key2']
            
            # ç®€å•ç­–ç•¥ï¼šä¿ç•™è¾ƒçŸ­çš„é”®åï¼Œæˆ–è€…å­—æ¯é¡ºåºè¾ƒå‰çš„
            if len(key1) < len(key2) or (len(key1) == len(key2) and key1 < key2):
                mapping[key2] = key1  # key2 -> key1
            else:
                mapping[key1] = key2  # key1 -> key2
        
        # å¤„ç†æœªä½¿ç”¨é”®å€¼ - æ ‡è®°ä¸ºåˆ é™¤
        for key in unused_keys:
            mapping[key] = "DELETE"
        
        # ä¿å­˜æ˜ å°„è¡¨
        with open("key_mappings.json", 'w', encoding='utf-8') as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é”®å€¼æ˜ å°„è¡¨å·²ç”Ÿæˆ: key_mappings.json")
        print(f"   - éœ€è¦åˆå¹¶çš„é”®å€¼: {len([k for k, v in mapping.items() if v != 'DELETE'])}")
        print(f"   - éœ€è¦åˆ é™¤çš„é”®å€¼: {len([k for k, v in mapping.items() if v == 'DELETE'])}")
        
        return mapping
    
    def optimize_arb_files(self, mapping=None):
        """æ ¹æ®æ˜ å°„è¡¨ä¼˜åŒ–ARBæ–‡ä»¶"""
        if not mapping:
            try:
                with open("key_mappings.json", 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
            except FileNotFoundError:
                print("âŒ æœªæ‰¾åˆ°æ˜ å°„è¡¨æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œåˆ†æ")
                return
        
        zh_data, en_data = self.load_arb_files()
        if not zh_data or not en_data:
            return
        
        # åˆ›å»ºå¤‡ä»½
        self.create_backup()
        
        # åº”ç”¨æ˜ å°„
        new_zh_data = OrderedDict()
        new_en_data = OrderedDict()
        
        # ä¿ç•™å…ƒæ•°æ®
        for key in zh_data:
            if key.startswith('@'):
                new_zh_data[key] = zh_data[key]
                if key in en_data:
                    new_en_data[key] = en_data[key]
        
        # å¤„ç†æ™®é€šé”®å€¼
        for key in zh_data:
            if key.startswith('@'):
                continue
                
            if key in mapping:
                target = mapping[key]
                if target == "DELETE":
                    print(f"åˆ é™¤é”®å€¼: {key}")
                    continue
                else:
                    print(f"åˆå¹¶é”®å€¼: {key} -> {target}")
                    # å¦‚æœç›®æ ‡é”®ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰é”®çš„å€¼
                    if target not in new_zh_data:
                        new_zh_data[target] = zh_data[key]
                        new_en_data[target] = en_data.get(key, '')
            else:
                # ä¿ç•™æœªæ˜ å°„çš„é”®å€¼
                new_zh_data[key] = zh_data[key]
                new_en_data[key] = en_data.get(key, '')
        
        # ä¿å­˜ä¼˜åŒ–åçš„æ–‡ä»¶
        with open(self.zh_arb_path, 'w', encoding='utf-8') as f:
            json.dump(new_zh_data, f, ensure_ascii=False, indent=2)
        
        with open(self.en_arb_path, 'w', encoding='utf-8') as f:
            json.dump(new_en_data, f, ensure_ascii=False, indent=2)
        
        print("âœ… ARBæ–‡ä»¶ä¼˜åŒ–å®Œæˆ")
        print(f"   åŸé”®å€¼æ•°é‡: {len([k for k in zh_data.keys() if not k.startswith('@')])}")
        print(f"   æ–°é”®å€¼æ•°é‡: {len([k for k in new_zh_data.keys() if not k.startswith('@')])}")

def main():
    parser = argparse.ArgumentParser(description='ARBæ–‡ä»¶ä¼˜åŒ–å™¨')
    parser.add_argument('--analyze', action='store_true', help='åˆ†æARBæ–‡ä»¶')
    parser.add_argument('--optimize', action='store_true', help='ä¼˜åŒ–ARBæ–‡ä»¶')
    parser.add_argument('--backup', action='store_true', help='åˆ›å»ºå¤‡ä»½')
    parser.add_argument('--generate-mapping', action='store_true', help='ç”Ÿæˆé”®å€¼æ˜ å°„è¡¨')
    parser.add_argument('--l10n-dir', default='lib/l10n', help='æœ¬åœ°åŒ–æ–‡ä»¶ç›®å½•')
    
    args = parser.parse_args()
    
    optimizer = ARBOptimizer(args.l10n_dir)
    
    if args.analyze:
        optimizer.analyze_arb_files()
    elif args.generate_mapping:
        zh_data, en_data = optimizer.load_arb_files()
        if zh_data and en_data:
            used_keys = optimizer.find_used_keys()
            duplicates = optimizer.find_duplicate_keys(zh_data, en_data)
            unused_keys = optimizer.find_unused_keys(zh_data, used_keys)
            optimizer.generate_key_mapping(duplicates, unused_keys)
    elif args.optimize:
        if args.backup:
            optimizer.create_backup()
        optimizer.optimize_arb_files()
    else:
        print("è¯·æŒ‡å®šæ“ä½œ: --analyze, --optimize, --generate-mapping")
        print("ä½¿ç”¨ --help æŸ¥çœ‹è¯¦ç»†è¯´æ˜")

if __name__ == "__main__":
    main()
