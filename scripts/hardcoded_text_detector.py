#!/usr/bin/env python3
"""
ç¡¬ç¼–ç æ–‡æœ¬æ£€æµ‹å™¨
ç”¨äºæ£€æµ‹Flutteré¡¹ç›®ä¸­çš„ç¡¬ç¼–ç ä¸­æ–‡æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§æ–‡æœ¬æ¨¡å¼è¯†åˆ«
"""

import os
import re
import json
import argparse
from collections import defaultdict
from dataclasses import dataclass, asdict
from typing import List, Dict, Set
import difflib

@dataclass
class HardcodedText:
    file_path: str
    line_number: int
    line_content: str
    text_content: str
    text_type: str
    context: str
    confidence: float = 1.0

class HardcodedTextDetector:
    def __init__(self):
        # æ£€æµ‹æ¨¡å¼ï¼šæ­£åˆ™è¡¨è¾¾å¼ -> æ–‡æœ¬ç±»å‹
        self.detection_patterns = {
            # Textç»„ä»¶
            r'Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[\),]': 'text_widget',
            r'Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*,': 'text_widget',
            
            # Text.rich
            r'TextSpan\s*\(\s*text\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[\),]': 'text_span',
            
            # SelectableText
            r'SelectableText\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[\),]': 'selectable_text',
            
            # AutoSizeText
            r'AutoSizeText\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[\),]': 'auto_size_text',
            
            # å±æ€§æ–‡æœ¬
            r'hintText\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'hint_text',
            r'labelText\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'label_text',
            r'helperText\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'helper_text',
            r'errorText\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'error_text',
            r'counterText\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'counter_text',
            r'placeholder\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'placeholder',
            r'tooltip\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'tooltip',
            r'semanticLabel\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'semantic_label',
            
            # titleå±æ€§
            r'title\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'title_text',
            r'title\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'title_string',
            
            # AppBar
            r'AppBar\s*\([^)]*title\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'app_bar_title',
            
            # Buttonæ–‡æœ¬
            r'child\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'button_text',
            r'label\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'label_text_widget',
            
            # Dialogç›¸å…³
            r'AlertDialog\s*\([^)]*title\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'alert_dialog_title',
            r'content\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'dialog_content',
            
            # SnackBar
            r'SnackBar\s*\([^)]*content\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'snackbar_content',
            r'content\s*:\s*Text\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'snackbar_content_simple',
            
            # æ¶ˆæ¯å’Œé€šçŸ¥
            r'message\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\)\}]': 'message',
            
            # è¿”å›è¯­å¥ä¸­çš„å­—ç¬¦ä¸²
            r'return\s+[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*;': 'return_string',
            
            # å¼‚å¸¸å’Œé”™è¯¯
            r'throw\s+\w+\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'exception_message',
            r'Exception\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'exception_constructor',
            
            # printå’Œæ—¥å¿—
            r'print\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'print_statement',
            r'log\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'log_statement',
            r'debugPrint\s*\(\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*\)': 'debug_print',
            
            # å­—ç¬¦ä¸²å¸¸é‡
            r'const\s+String\s+\w+\s*=\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*;': 'string_constant',
            r'final\s+String\s+\w+\s*=\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*;': 'string_final',
            r'String\s+\w+\s*=\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*;': 'string_variable',
            
            # æšä¸¾æ˜¾ç¤ºåç§°ç›¸å…³
            r'case\s+\w+\.\w+\s*:\s*return\s+[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*;': 'enum_display_name',
            
            # Mapä¸­çš„å€¼
            r'[\'\"]\w*[\'\"]\s*:\s*[\'\"](.*?[\u4e00-\u9fff].*?)[\'\"]\s*[,\}]': 'map_value',
        }
        
        self.exclude_patterns = [
            r'//.*?[\u4e00-\u9fff].*',  # æ³¨é‡Š
            r'/\*.*?[\u4e00-\u9fff].*?\*/',  # å¤šè¡Œæ³¨é‡Š
            r'TODO.*?[\u4e00-\u9fff].*',  # TODOæ³¨é‡Š
        ]
        
        self.exclude_files = [
            'generated',
            '.g.dart',
            '.freezed.dart',
            '.gr.dart',
            'app_localizations.dart',
            'app_localizations_',
        ]
    
    def should_exclude_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ’é™¤æ–‡ä»¶"""
        for pattern in self.exclude_files:
            if pattern in file_path:
                return True
        return False
    
    def is_in_comment(self, line: str, match_start: int) -> bool:
        """åˆ¤æ–­åŒ¹é…æ˜¯å¦åœ¨æ³¨é‡Šä¸­"""
        # æ£€æŸ¥å•è¡Œæ³¨é‡Š
        comment_pos = line.find('//')
        if comment_pos != -1 and comment_pos < match_start:
            return True
        
        # ç®€å•æ£€æŸ¥æ˜¯å¦åœ¨å¤šè¡Œæ³¨é‡Šä¸­ï¼ˆä¸å®Œç¾ï¼Œä½†åŸºæœ¬å¤Ÿç”¨ï¼‰
        if '/*' in line[:match_start] and '*/' not in line[match_start:]:
            return True
            
        return False
    
    def extract_context(self, file_path: str, line_number: int, lines: List[str]) -> str:
        """æå–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []
        
        # ä»æ–‡ä»¶è·¯å¾„æå–æ¨¡å—ä¿¡æ¯
        path_parts = file_path.replace('\\', '/').split('/')
        if len(path_parts) > 2:
            context_parts.append(f"æ¨¡å—:{path_parts[-2]}")
        
        # æŸ¥æ‰¾å½“å‰å‡½æ•°æˆ–ç±»
        function_name = self.find_current_function(line_number, lines)
        if function_name:
            context_parts.append(f"å‡½æ•°:{function_name}")
        
        class_name = self.find_current_class(line_number, lines)
        if class_name:
            context_parts.append(f"ç±»:{class_name}")
        
        return " ".join(context_parts)
    
    def find_current_function(self, line_number: int, lines: List[str]) -> str:
        """æŸ¥æ‰¾å½“å‰æ‰€åœ¨å‡½æ•°"""
        for i in range(line_number - 1, max(0, line_number - 20), -1):
            if i < len(lines):
                line = lines[i].strip()
                # åŒ¹é…å‡½æ•°å®šä¹‰
                func_match = re.search(r'(?:Future<.*?>|void|String|int|bool|Widget|\w+)\s+(\w+)\s*\(', line)
                if func_match:
                    return func_match.group(1)
        return ""
    
    def find_current_class(self, line_number: int, lines: List[str]) -> str:
        """æŸ¥æ‰¾å½“å‰æ‰€åœ¨ç±»"""
        for i in range(line_number - 1, max(0, line_number - 50), -1):
            if i < len(lines):
                line = lines[i].strip()
                # åŒ¹é…ç±»å®šä¹‰
                class_match = re.search(r'class\s+(\w+)', line)
                if class_match:
                    return class_match.group(1)
        return ""
    
    def detect_in_file(self, file_path: str) -> List[HardcodedText]:
        """æ£€æµ‹å•ä¸ªæ–‡ä»¶ä¸­çš„ç¡¬ç¼–ç æ–‡æœ¬"""
        if self.should_exclude_file(file_path):
            return []
        
        results = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            for line_num, line in enumerate(lines, 1):
                # è·³è¿‡ç©ºè¡Œå’Œçº¯ç©ºç™½è¡Œ
                if not line.strip():
                    continue
                
                for pattern, text_type in self.detection_patterns.items():
                    matches = re.finditer(pattern, line, re.DOTALL)
                    
                    for match in matches:
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ³¨é‡Šä¸­
                        if self.is_in_comment(line, match.start()):
                            continue
                        
                        text_content = match.group(1)
                        
                        # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯ç”¨æˆ·ç•Œé¢æ–‡æœ¬çš„å†…å®¹
                        if self.should_skip_text(text_content):
                            continue
                        
                        context = self.extract_context(file_path, line_num, lines)
                        
                        hardcoded_text = HardcodedText(
                            file_path=file_path,
                            line_number=line_num,
                            line_content=line.strip(),
                            text_content=text_content,
                            text_type=text_type,
                            context=context,
                            confidence=self.calculate_confidence(text_content, text_type)
                        )
                        
                        results.append(hardcoded_text)
        
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return results
    
    def should_skip_text(self, text: str) -> str:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸ªæ–‡æœ¬"""
        # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
        if len(text.strip()) < 2:
            return True
        
        # è·³è¿‡URL
        if text.startswith('http') or text.startswith('www'):
            return True
        
        # è·³è¿‡æ–‡ä»¶è·¯å¾„
        if '/' in text or '\\' in text:
            return True
        
        # è·³è¿‡åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æŠ€æœ¯æ€§æ–‡æœ¬
        technical_chars = ['${', '}', '<', '>', '[', ']', '{', '}']
        if any(char in text for char in technical_chars):
            return True
        
        return False
    
    def calculate_confidence(self, text: str, text_type: str) -> float:
        """è®¡ç®—æ£€æµ‹ç½®ä¿¡åº¦"""
        confidence = 1.0
        
        # æ ¹æ®æ–‡æœ¬ç±»å‹è°ƒæ•´ç½®ä¿¡åº¦
        high_confidence_types = ['text_widget', 'button_text', 'title_text', 'hint_text']
        if text_type in high_confidence_types:
            confidence = 1.0
        elif text_type in ['print_statement', 'debug_print', 'log_statement']:
            confidence = 0.5  # æ—¥å¿—å¯èƒ½ä¸éœ€è¦å›½é™…åŒ–
        elif text_type == 'return_string':
            confidence = 0.8
        
        # æ ¹æ®æ–‡æœ¬é•¿åº¦è°ƒæ•´
        if len(text) < 3:
            confidence *= 0.6
        elif len(text) > 50:
            confidence *= 0.8
        
        return confidence
    
    def scan_all_files(self, root_dir: str = "lib") -> List[HardcodedText]:
        """æ‰«ææ‰€æœ‰Dartæ–‡ä»¶"""
        all_results = []
        dart_files = []
        
        # æ”¶é›†æ‰€æœ‰Dartæ–‡ä»¶
        for root, dirs, files in os.walk(root_dir):
            # è·³è¿‡ç”Ÿæˆçš„æ–‡ä»¶ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != 'generated']
            
            for file in files:
                if file.endswith('.dart'):
                    dart_files.append(os.path.join(root, file))
        
        print(f"ğŸ” å¼€å§‹æ‰«æ {len(dart_files)} ä¸ªDartæ–‡ä»¶...")
        
        for i, file_path in enumerate(dart_files, 1):
            if i % 10 == 0:
                print(f"   è¿›åº¦: {i}/{len(dart_files)}")
            
            results = self.detect_in_file(file_path)
            all_results.extend(results)
        
        return all_results
    
    def group_by_file(self, results: List[HardcodedText]) -> Dict[str, List[HardcodedText]]:
        """æŒ‰æ–‡ä»¶åˆ†ç»„ç»“æœ"""
        grouped = defaultdict(list)
        for result in results:
            grouped[result.file_path].append(result)
        return dict(grouped)
    
    def group_by_type(self, results: List[HardcodedText]) -> Dict[str, List[HardcodedText]]:
        """æŒ‰ç±»å‹åˆ†ç»„ç»“æœ"""
        grouped = defaultdict(list)
        for result in results:
            grouped[result.text_type].append(result)
        return dict(grouped)
    
    def generate_report(self, results: List[HardcodedText], output_file: str = "hardcoded_text_report.md"):
        """ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
        grouped_by_file = self.group_by_file(results)
        grouped_by_type = self.group_by_type(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# ç¡¬ç¼–ç æ–‡æœ¬æ£€æµ‹æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            f.write("## ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- æ£€æµ‹åˆ°ç¡¬ç¼–ç æ–‡æœ¬: {len(results)} å¤„\n")
            f.write(f"- æ¶‰åŠæ–‡ä»¶: {len(grouped_by_file)} ä¸ª\n")
            f.write(f"- æ–‡æœ¬ç±»å‹: {len(grouped_by_type)} ç§\n\n")
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            f.write("## æŒ‰ç±»å‹ç»Ÿè®¡\n\n")
            for text_type, items in sorted(grouped_by_type.items(), key=lambda x: len(x[1]), reverse=True):
                f.write(f"- **{text_type}**: {len(items)} å¤„\n")
            f.write("\n")
            
            # æŒ‰æ–‡ä»¶è¯¦æƒ…
            f.write("## æŒ‰æ–‡ä»¶è¯¦æƒ…\n\n")
            for file_path, items in sorted(grouped_by_file.items(), key=lambda x: len(x[1]), reverse=True):
                f.write(f"### {file_path} ({len(items)} å¤„)\n\n")
                
                for item in items:
                    f.write(f"**ç¬¬ {item.line_number} è¡Œ** ({item.text_type}):\n")
                    f.write(f"- æ–‡æœ¬: `{item.text_content}`\n")
                    f.write(f"- ä»£ç : `{item.line_content}`\n")
                    if item.context:
                        f.write(f"- ä¸Šä¸‹æ–‡: {item.context}\n")
                    f.write(f"- ç½®ä¿¡åº¦: {item.confidence:.2f}\n\n")
        
        print(f"âœ… æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    
    def export_json(self, results: List[HardcodedText], output_file: str = "hardcoded_texts.json"):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        data = [asdict(result) for result in results]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONæ•°æ®å·²å¯¼å‡º: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='ç¡¬ç¼–ç æ–‡æœ¬æ£€æµ‹å™¨')
    parser.add_argument('--scan', action='store_true', help='æ‰«æç¡¬ç¼–ç æ–‡æœ¬')
    parser.add_argument('--root-dir', default='lib', help='æ‰«ææ ¹ç›®å½•')
    parser.add_argument('--output', default='hardcoded_text_report.md', help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--json', action='store_true', help='åŒæ—¶å¯¼å‡ºJSONæ ¼å¼')
    parser.add_argument('--min-confidence', type=float, default=0.5, help='æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼')
    
    args = parser.parse_args()
    
    if args.scan:
        detector = HardcodedTextDetector()
        results = detector.scan_all_files(args.root_dir)
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
        filtered_results = [r for r in results if r.confidence >= args.min_confidence]
        
        print(f"\nğŸ“Š æ£€æµ‹ç»“æœ:")
        print(f"   æ€»è®¡: {len(results)} å¤„")
        print(f"   é«˜ç½®ä¿¡åº¦ (>={args.min_confidence}): {len(filtered_results)} å¤„")
        
        if filtered_results:
            detector.generate_report(filtered_results, args.output)
            
            if args.json:
                json_file = args.output.replace('.md', '.json')
                detector.export_json(filtered_results, json_file)
        else:
            print("âœ… æœªæ£€æµ‹åˆ°éœ€è¦å¤„ç†çš„ç¡¬ç¼–ç æ–‡æœ¬")
    else:
        print("è¯·ä½¿ç”¨ --scan å¼€å§‹æ‰«æ")
        print("ä½¿ç”¨ --help æŸ¥çœ‹è¯¦ç»†è¯´æ˜")

if __name__ == "__main__":
    import datetime
    main()
