#!/usr/bin/env python3
"""
æ™ºèƒ½ARBé”®å€¼åŒ¹é…å™¨
ç”¨äºä¸ºç¡¬ç¼–ç æ–‡æœ¬åŒ¹é…ç°æœ‰ARBé”®å€¼æˆ–ç”Ÿæˆæ–°çš„é”®å€¼å»ºè®®
"""

import os
import json
import re
import argparse
from typing import Dict, List, Tuple, Optional
from difflib import SequenceMatcher
from collections import defaultdict
import jieba  # éœ€è¦å®‰è£…: pip install jieba

class SmartARBMatcher:
    def __init__(self, arb_zh_path: str = "lib/l10n/app_zh.arb", arb_en_path: str = "lib/l10n/app_en.arb"):
        self.arb_zh_path = arb_zh_path
        self.arb_en_path = arb_en_path
        self.zh_entries = {}
        self.en_entries = {}
        self.load_arb_files()
        
        # æ¨¡å—æ˜ å°„ï¼šè·¯å¾„å…³é”®è¯ -> æ¨¡å—å‰ç¼€
        self.module_mapping = {
            'auth': 'auth',
            'login': 'auth',
            'register': 'auth',
            'home': 'home',
            'main': 'home',
            'settings': 'settings',
            'profile': 'profile',
            'library': 'library',
            'practice': 'practice',
            'edit': 'edit',
            'dialog': 'dialog',
            'widget': 'widget',
            'component': 'component',
            'page': 'page',
        }
        
        # ç»„ä»¶ç±»å‹æ˜ å°„
        self.component_mapping = {
            'text_widget': 'text',
            'button_text': 'button',
            'title_text': 'title',
            'hint_text': 'hint',
            'error_text': 'error',
            'label_text': 'label',
            'dialog_content': 'dialog',
            'snackbar_content': 'message',
            'tooltip': 'tooltip',
            'placeholder': 'placeholder',
        }
        
        # è¯­ä¹‰å…³é”®è¯æ˜ å°„
        self.semantic_keywords = {
            'ä¿å­˜': 'save',
            'åˆ é™¤': 'delete',
            'å–æ¶ˆ': 'cancel',
            'ç¡®è®¤': 'confirm',
            'æäº¤': 'submit',
            'è¿”å›': 'back',
            'ä¸‹ä¸€æ­¥': 'next',
            'ä¸Šä¸€æ­¥': 'previous',
            'å®Œæˆ': 'complete',
            'å¼€å§‹': 'start',
            'ç»“æŸ': 'end',
            'ç™»å½•': 'login',
            'æ³¨å†Œ': 'register',
            'é€€å‡º': 'logout',
            'æœç´¢': 'search',
            'è®¾ç½®': 'settings',
            'å¸®åŠ©': 'help',
            'å…³äº': 'about',
            'æˆåŠŸ': 'success',
            'å¤±è´¥': 'failed',
            'é”™è¯¯': 'error',
            'è­¦å‘Š': 'warning',
            'æç¤º': 'tip',
            'æ¶ˆæ¯': 'message',
        }
    
    def load_arb_files(self):
        """åŠ è½½ARBæ–‡ä»¶"""
        try:
            with open(self.arb_zh_path, 'r', encoding='utf-8') as f:
                zh_data = json.load(f)
                self.zh_entries = {k: v for k, v in zh_data.items() if not k.startswith('@')}
            
            with open(self.arb_en_path, 'r', encoding='utf-8') as f:
                en_data = json.load(f)
                self.en_entries = {k: v for k, v in en_data.items() if not k.startswith('@')}
                
            print(f"âœ… å·²åŠ è½½ {len(self.zh_entries)} ä¸ªARBé”®å€¼")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ARBæ–‡ä»¶å¤±è´¥: {e}")
    
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        clean_text1 = re.sub(r'[^\w\u4e00-\u9fff]', '', text1)
        clean_text2 = re.sub(r'[^\w\u4e00-\u9fff]', '', text2)
        
        # è®¡ç®—å­—ç¬¦çº§ç›¸ä¼¼åº¦
        char_similarity = SequenceMatcher(None, clean_text1, clean_text2).ratio()
        
        # è®¡ç®—è¯çº§ç›¸ä¼¼åº¦ï¼ˆé’ˆå¯¹ä¸­æ–‡ï¼‰
        words1 = set(jieba.cut(clean_text1))
        words2 = set(jieba.cut(clean_text2))
        
        if words1 and words2:
            word_similarity = len(words1 & words2) / len(words1 | words2)
        else:
            word_similarity = 0
        
        # ç»¼åˆç›¸ä¼¼åº¦
        return (char_similarity * 0.6 + word_similarity * 0.4)
    
    def find_similar_keys(self, text: str, threshold: float = 0.7) -> List[Tuple[str, str, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„ç°æœ‰é”®å€¼"""
        similar_keys = []
        
        for key, value in self.zh_entries.items():
            similarity = self.calculate_text_similarity(text, value)
            if similarity >= threshold:
                similar_keys.append((key, value, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_keys.sort(key=lambda x: x[2], reverse=True)
        return similar_keys[:5]  # è¿”å›å‰5ä¸ªæœ€ç›¸ä¼¼çš„
    
    def extract_module_from_path(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–æ¨¡å—ä¿¡æ¯"""
        path_lower = file_path.lower().replace('\\', '/')
        
        for keyword, module in self.module_mapping.items():
            if keyword in path_lower:
                return module
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œä½¿ç”¨çˆ¶ç›®å½•å
        parts = path_lower.split('/')
        if len(parts) >= 2:
            parent_dir = parts[-2]
            # æ¸…ç†ç›®å½•å
            clean_dir = re.sub(r'[^a-z]', '', parent_dir)
            if clean_dir:
                return clean_dir[:8]  # é™åˆ¶é•¿åº¦
        
        return 'common'
    
    def extract_component_type(self, text_type: str) -> str:
        """æå–ç»„ä»¶ç±»å‹"""
        return self.component_mapping.get(text_type, 'text')
    
    def extract_semantic_meaning(self, text: str) -> str:
        """æå–è¯­ä¹‰å«ä¹‰"""
        text_clean = re.sub(r'[^\w\u4e00-\u9fff]', '', text)
        
        # æŸ¥æ‰¾å…³é”®è¯
        for keyword, semantic in self.semantic_keywords.items():
            if keyword in text:
                return semantic
        
        # ä½¿ç”¨åˆ†è¯æå–ä¸»è¦è¯æ±‡
        words = list(jieba.cut(text_clean))
        # è¿‡æ»¤åœç”¨è¯å’Œå•å­—ç¬¦
        meaningful_words = [w for w in words if len(w) > 1 and w not in ['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'éƒ½', 'ä¸']]
        
        if meaningful_words:
            # é€‰æ‹©æœ€é•¿çš„è¯ä½œä¸ºè¯­ä¹‰æ ‡è¯†
            main_word = max(meaningful_words, key=len)
            # è½¬æ¢ä¸ºæ‹¼éŸ³æˆ–è‹±æ–‡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            return self.chinese_to_pinyin(main_word)
        
        return 'content'
    
    def chinese_to_pinyin(self, text: str) -> str:
        """ç®€åŒ–çš„ä¸­æ–‡åˆ°æ‹¼éŸ³è½¬æ¢"""
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„æ˜ å°„ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨ pypinyin åº“
        pinyin_map = {
            'ä¿å­˜': 'save', 'åˆ é™¤': 'delete', 'å–æ¶ˆ': 'cancel', 'ç¡®è®¤': 'confirm',
            'æäº¤': 'submit', 'è¿”å›': 'back', 'æœç´¢': 'search', 'è®¾ç½®': 'settings',
            'ç”¨æˆ·': 'user', 'å¯†ç ': 'password', 'é‚®ç®±': 'email', 'æ‰‹æœº': 'phone',
            'å§“å': 'name', 'åœ°å€': 'address', 'å¹´é¾„': 'age', 'æ€§åˆ«': 'gender',
            'æ–‡ä»¶': 'file', 'å›¾ç‰‡': 'image', 'è§†é¢‘': 'video', 'éŸ³é¢‘': 'audio',
            'æ ‡é¢˜': 'title', 'å†…å®¹': 'content', 'æè¿°': 'description',
            'æ—¶é—´': 'time', 'æ—¥æœŸ': 'date', 'ä½ç½®': 'location',
            'é¡µé¢': 'page', 'èœå•': 'menu', 'æŒ‰é’®': 'button', 'è¾“å…¥': 'input',
        }
        
        # æŸ¥æ‰¾å®Œå…¨åŒ¹é…
        if text in pinyin_map:
            return pinyin_map[text]
        
        # æŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…
        for chinese, english in pinyin_map.items():
            if chinese in text:
                return english
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨æ•°å­—ä½œä¸ºåç¼€
        import hashlib
        hash_value = hashlib.md5(text.encode()).hexdigest()[:4]
        return f'text_{hash_value}'
    
    def suggest_key_name(self, text: str, file_path: str, text_type: str, context: str = '') -> str:
        """å»ºè®®æ–°çš„é”®å"""
        module = self.extract_module_from_path(file_path)
        component = self.extract_component_type(text_type)
        semantic = self.extract_semantic_meaning(text)
        
        # æ„å»ºé”®å
        key_parts = [module, component, semantic]
        
        # ä»ä¸Šä¸‹æ–‡ä¸­æå–é¢å¤–ä¿¡æ¯
        if context:
            context_lower = context.lower()
            if 'dialog' in context_lower:
                key_parts.insert(-1, 'dialog')
            elif 'form' in context_lower:
                key_parts.insert(-1, 'form')
        
        suggested_key = '_'.join(key_parts)
        
        # ç¡®ä¿é”®åå”¯ä¸€
        original_key = suggested_key
        counter = 1
        while suggested_key in self.zh_entries:
            suggested_key = f"{original_key}_{counter}"
            counter += 1
        
        return suggested_key
    
    def match_or_suggest(self, text: str, file_path: str, text_type: str, context: str = '') -> Dict:
        """åŒ¹é…ç°æœ‰é”®å€¼æˆ–å»ºè®®æ–°é”®å€¼"""
        # é¦–å…ˆæŸ¥æ‰¾ç›¸ä¼¼çš„ç°æœ‰é”®å€¼
        similar_keys = self.find_similar_keys(text)
        
        result = {
            'text': text,
            'file_path': file_path,
            'text_type': text_type,
            'context': context,
            'similar_keys': similar_keys,
            'suggested_key': None,
            'action': 'unknown'
        }
        
        if similar_keys and similar_keys[0][2] > 0.85:
            # é«˜ç›¸ä¼¼åº¦ï¼Œå»ºè®®å¤ç”¨
            result['action'] = 'reuse'
            result['recommended_key'] = similar_keys[0][0]
            result['recommended_text'] = similar_keys[0][1]
            result['similarity'] = similar_keys[0][2]
        else:
            # å»ºè®®åˆ›å»ºæ–°é”®å€¼
            result['action'] = 'create'
            result['suggested_key'] = self.suggest_key_name(text, file_path, text_type, context)
        
        return result
    
    def batch_match(self, hardcoded_texts: List[Dict]) -> List[Dict]:
        """æ‰¹é‡åŒ¹é…ç¡¬ç¼–ç æ–‡æœ¬"""
        results = []
        
        print(f"ğŸ” å¼€å§‹åŒ¹é… {len(hardcoded_texts)} ä¸ªç¡¬ç¼–ç æ–‡æœ¬...")
        
        for i, item in enumerate(hardcoded_texts, 1):
            if i % 20 == 0:
                print(f"   è¿›åº¦: {i}/{len(hardcoded_texts)}")
            
            result = self.match_or_suggest(
                item['text_content'],
                item['file_path'],
                item['text_type'],
                item.get('context', '')
            )
            
            # æ·»åŠ åŸå§‹ä¿¡æ¯
            result.update({
                'line_number': item['line_number'],
                'line_content': item['line_content'],
                'confidence': item.get('confidence', 1.0)
            })
            
            results.append(result)
        
        return results
    
    def generate_arb_additions(self, results: List[Dict], output_file: str = "arb_additions.json"):
        """ç”Ÿæˆéœ€è¦æ·»åŠ åˆ°ARBçš„æ–°é”®å€¼"""
        additions = {
            'zh': {},
            'en': {}
        }
        
        for result in results:
            if result['action'] == 'create' and result['suggested_key']:
                key = result['suggested_key']
                zh_text = result['text']
                
                # ç®€å•çš„è‹±æ–‡ç¿»è¯‘ï¼ˆå®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨ä¸“ä¸šç¿»è¯‘ï¼‰
                en_text = self.simple_translate_to_english(zh_text)
                
                additions['zh'][key] = zh_text
                additions['en'][key] = en_text
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(additions, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ARBæ–°å¢é”®å€¼å·²ç”Ÿæˆ: {output_file}")
        print(f"   éœ€è¦æ·»åŠ  {len(additions['zh'])} ä¸ªæ–°é”®å€¼")
        
        return additions
    
    def simple_translate_to_english(self, chinese_text: str) -> str:
        """ç®€å•çš„ä¸­è‹±æ–‡ç¿»è¯‘ï¼ˆå®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨ä¸“ä¸šç¿»è¯‘æœåŠ¡ï¼‰"""
        # ç®€å•çš„ç¿»è¯‘æ˜ å°„
        translation_map = {
            'ä¿å­˜': 'Save',
            'åˆ é™¤': 'Delete',
            'å–æ¶ˆ': 'Cancel',
            'ç¡®è®¤': 'Confirm',
            'æäº¤': 'Submit',
            'è¿”å›': 'Back',
            'æœç´¢': 'Search',
            'è®¾ç½®': 'Settings',
            'ç™»å½•': 'Login',
            'æ³¨å†Œ': 'Register',
            'é€€å‡º': 'Logout',
            'æˆåŠŸ': 'Success',
            'å¤±è´¥': 'Failed',
            'é”™è¯¯': 'Error',
            'è­¦å‘Š': 'Warning',
            'æç¤º': 'Tip',
            'æ¶ˆæ¯': 'Message',
            'å¼€å§‹': 'Start',
            'ç»“æŸ': 'End',
            'å®Œæˆ': 'Complete',
            'å¸®åŠ©': 'Help',
            'å…³äº': 'About',
            'ç”¨æˆ·': 'User',
            'å¯†ç ': 'Password',
            'é‚®ç®±': 'Email',
            'æ‰‹æœº': 'Phone',
            'å§“å': 'Name',
            'åœ°å€': 'Address',
            'æ–‡ä»¶': 'File',
            'å›¾ç‰‡': 'Image',
            'è¯·è¾“å…¥': 'Please enter',
            'è¯·é€‰æ‹©': 'Please select',
            'åŠ è½½ä¸­': 'Loading',
            'æš‚æ— æ•°æ®': 'No data',
            'ç½‘ç»œé”™è¯¯': 'Network error',
            'æ“ä½œæˆåŠŸ': 'Operation successful',
            'æ“ä½œå¤±è´¥': 'Operation failed',
        }
        
        # æŸ¥æ‰¾ç›´æ¥åŒ¹é…
        if chinese_text in translation_map:
            return translation_map[chinese_text]
        
        # æŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…å¹¶æ›¿æ¢
        result = chinese_text
        for chinese, english in translation_map.items():
            if chinese in result:
                result = result.replace(chinese, english)
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•åŒ¹é…ï¼Œæ ‡è®°éœ€è¦äººå·¥ç¿»è¯‘
        if result == chinese_text:
            return f"[TODO: Translate '{chinese_text}']"
        
        return result
    
    def generate_match_report(self, results: List[Dict], output_file: str = "arb_match_report.md"):
        """ç”ŸæˆåŒ¹é…æŠ¥å‘Š"""
        reuse_count = sum(1 for r in results if r['action'] == 'reuse')
        create_count = sum(1 for r in results if r['action'] == 'create')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# ARBé”®å€¼åŒ¹é…æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            f.write("## ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- æ€»è®¡ç¡¬ç¼–ç æ–‡æœ¬: {len(results)} å¤„\n")
            f.write(f"- å¯å¤ç”¨ç°æœ‰é”®å€¼: {reuse_count} å¤„\n")
            f.write(f"- éœ€è¦åˆ›å»ºæ–°é”®å€¼: {create_count} å¤„\n")
            f.write(f"- å¤ç”¨ç‡: {reuse_count/len(results)*100:.1f}%\n\n")
            
            # å¯å¤ç”¨çš„é”®å€¼
            if reuse_count > 0:
                f.write("## å¯å¤ç”¨ç°æœ‰é”®å€¼\n\n")
                for result in results:
                    if result['action'] == 'reuse':
                        f.write(f"### {result['file_path']}:{result['line_number']}\n\n")
                        f.write(f"- **ç¡¬ç¼–ç æ–‡æœ¬**: `{result['text']}`\n")
                        f.write(f"- **æ¨èé”®å€¼**: `{result['recommended_key']}`\n")
                        f.write(f"- **é”®å€¼æ–‡æœ¬**: `{result['recommended_text']}`\n")
                        f.write(f"- **ç›¸ä¼¼åº¦**: {result['similarity']:.2f}\n")
                        f.write(f"- **ä»£ç è¡Œ**: `{result['line_content']}`\n\n")
            
            # éœ€è¦åˆ›å»ºçš„æ–°é”®å€¼
            if create_count > 0:
                f.write("## éœ€è¦åˆ›å»ºçš„æ–°é”®å€¼\n\n")
                for result in results:
                    if result['action'] == 'create':
                        f.write(f"### {result['file_path']}:{result['line_number']}\n\n")
                        f.write(f"- **ç¡¬ç¼–ç æ–‡æœ¬**: `{result['text']}`\n")
                        f.write(f"- **å»ºè®®é”®å**: `{result['suggested_key']}`\n")
                        f.write(f"- **ä»£ç è¡Œ**: `{result['line_content']}`\n")
                        if result['similar_keys']:
                            f.write("- **ç›¸ä¼¼çš„ç°æœ‰é”®å€¼**:\n")
                            for key, text, sim in result['similar_keys'][:3]:
                                f.write(f"  - `{key}`: {text} (ç›¸ä¼¼åº¦: {sim:.2f})\n")
                        f.write("\n")
        
        print(f"âœ… åŒ¹é…æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='æ™ºèƒ½ARBé”®å€¼åŒ¹é…å™¨')
    parser.add_argument('--input', required=True, help='ç¡¬ç¼–ç æ–‡æœ¬JSONæ–‡ä»¶')
    parser.add_argument('--arb-zh', default='lib/l10n/app_zh.arb', help='ä¸­æ–‡ARBæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--arb-en', default='lib/l10n/app_en.arb', help='è‹±æ–‡ARBæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--report', default='arb_match_report.md', help='åŒ¹é…æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--additions', default='arb_additions.json', help='æ–°é”®å€¼è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--threshold', type=float, default=0.7, help='ç›¸ä¼¼åº¦é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # åŠ è½½ç¡¬ç¼–ç æ–‡æœ¬æ•°æ®
    try:
        with open(args.input, 'r', encoding='utf-8') as f:
            hardcoded_texts = json.load(f)
        print(f"âœ… å·²åŠ è½½ {len(hardcoded_texts)} ä¸ªç¡¬ç¼–ç æ–‡æœ¬")
    except Exception as e:
        print(f"âŒ åŠ è½½ç¡¬ç¼–ç æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºåŒ¹é…å™¨
    matcher = SmartARBMatcher(args.arb_zh, args.arb_en)
    
    # æ‰§è¡ŒåŒ¹é…
    results = matcher.batch_match(hardcoded_texts)
    
    # ç”ŸæˆæŠ¥å‘Š
    matcher.generate_match_report(results, args.report)
    
    # ç”Ÿæˆæ–°é”®å€¼
    matcher.generate_arb_additions(results, args.additions)
    
    # è¾“å‡ºç»Ÿè®¡
    reuse_count = sum(1 for r in results if r['action'] == 'reuse')
    create_count = sum(1 for r in results if r['action'] == 'create')
    
    print(f"\nğŸ“Š åŒ¹é…ç»“æœ:")
    print(f"   å¯å¤ç”¨ç°æœ‰é”®å€¼: {reuse_count} å¤„")
    print(f"   éœ€è¦åˆ›å»ºæ–°é”®å€¼: {create_count} å¤„")
    print(f"   å¤ç”¨ç‡: {reuse_count/len(results)*100:.1f}%")

if __name__ == "__main__":
    import datetime
    main()
